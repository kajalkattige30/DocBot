# -*- coding: utf-8 -*-
"""Final copy of nlp_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J6xkVFq2_2AIi5Lun_Ayrk9Bz0Tw3K4Q
"""

# pip install newspaper3k


# pip install nltk

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('stopwords')

# from google.colab import drive
# drive.mount('/content/drive')

from newspaper import Article
import random
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
import numpy as np
import warnings

warnings.filterwarnings('ignore')

nltk.download('punkt', quiet = True)
nltk.download('wordnet', quiet = True)

remove_punct_dict = dict( (ord(punct),None) for punct in string.punctuation)
print(string.punctuation)
print(remove_punct_dict)

# Webscraping dieases list and links from WHO site


from urllib.request import urlopen as uReq
from bs4 import BeautifulSoup as soup

my_url = "https://www.who.int/news-room/fact-sheets"

uClient = uReq(my_url)
page_html = uClient.read()
uClient.close()
page_soup = soup(page_html,"html.parser")

ul = page_soup.find('ul',{'id':'alphabetical-nav-filter'})

aAll = ul.findAll('a')

diseasesArray = []
diseaseNames = []
diseaseLinks = []
diseasesDict = dict()
for a in aAll:
  item = []
  diseaseName = a.text
  diseaseLink = a['href']
  item.append(diseaseName)
  diseaseNames.append(diseaseName)
  diseaseLinks.append(diseaseLink)
  item.append(diseaseLink)
  diseasesArray.append(item)
  diseasesDict[diseaseName] = diseaseLink

# Generating Corpus
def generateCorpus(path):
  article = Article("https://www.who.int"+path)
  article.download()
  article.parse()
  article.nlp()
  corpus = article.text
  return corpus

#Tokenization of the corpus
def sentTokenize(corpus):
  text = corpus
  sent_tokens = nltk.sent_tokenize(text)
  return sent_tokens

#Removing Punctutations from the text
def LemNormalize(text):
  return nltk.word_tokenize(text.lower().translate(remove_punct_dict))

#Greeting Response

greeting_inputs = ["hi", "hello", "hola", "greetings", "wassup", "hey"]
greeting_responses = ["howdy" , "hi" , "hey", "what's good", "hello", "hello" , "hey there"]
disease_ques = ["Hello! Which disease do you want to enquire about?", "Heyya! Which disease's information do you want?" ,"Hey, which disease do you want to know about?" , "Hello! Which disease do you want to learn about?"]

def greeting(sentence):
  for word in sentence.split():
    if word.lower() in greeting_inputs:
      return random.choice(disease_ques)

# TFIDF and Cosine Similarity Response

def response(user_response,sent_tokens):

  user_response = user_response.lower()
  robo_response = ''
  sent_tokens.append(user_response)
  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words='english')
  tfidf = TfidfVec.fit_transform(sent_tokens)
  vals = cosine_similarity(tfidf[-1], tfidf)
  idx = vals.argsort()[0][-2]
  flat = vals.flatten()
  flat.sort()
  score = flat[-2]
  if(score == 0):
    robo_response = robo_response+"I apologize, I don't understand."
  else:
    if('?' not in sent_tokens[idx]):
      robo_response = robo_response+sent_tokens[idx]
    else:
      robo_response = robo_response+sent_tokens[idx+1]
  sent_tokens.remove(user_response)
  return idx,robo_response

# Finding the link corresponding to the disease in user query

def which_link(user_response):
  l = []
  
  for i in diseasesDict:
    i_list = word_tokenize(i.lower().translate(remove_punct_dict))
    q_list = word_tokenize(user_response.lower().translate(remove_punct_dict))
    sw = stopwords.words('english') 
    l1 =[];l2 =[]
    i_set = {w for w in i_list if not w in sw}  
    q_set = {w for w in q_list if not w in sw}
    rvector = i_set.union(q_set)  
    for w in rvector: 
      if w in i_set: 
        l1.append(1)
      else: 
        l1.append(0) 
      if w in q_set:
         l2.append(1) 
      else: 
        l2.append(0) 
    c = 0
    for i in range(len(rvector)): 
        c+= l1[i]*l2[i] 
    cosine = c / float((sum(l1)*sum(l2))**0.5) 
    l.append(cosine)
    
  return(l.index(max(l)))

# Creating kernel of AIML file

# !pip install aiml
import aiml

kernel = aiml.Kernel()
kernel.learn("C:/Users/kkatt/Documents/NLP Project/healthbot.aiml")

# from google.colab import drive
# drive.mount('/content/drive')

flag = True
print("Hi, I'm DocBot!")
previousIDX = 0
previousResponse = ""
link = ""
path = ""
sent_tokens = []
while(flag == True):

  #Take the user input
  user_response = input()

  #Clean the user input
  user_response = user_response.lower()
  exclude = set(string.punctuation)
  user_response = ''.join(ch for ch in user_response if ch not in exclude)

  #To stop the conversation
  if(user_response != 'bye'):

    #Response for Thanks
    if(user_response == 'thanks' or user_response == 'thank you'):
      flag = False
      print("DocBot: You are Welcome!")

    #Greetings Response
    elif(greeting(user_response) != None):
        print("DocBot: "+greeting(user_response))

    #Search for user query in AIML 
    elif(kernel.respond(user_response) != "no match"):
        print("DocBot: "+kernel.respond(user_response))

    #Search more functionality
    elif(user_response == "search more"):
      sent_tokens.remove(sent_tokens[previousIDX])
      previousIDX,answer = response(previousResponse,sent_tokens) 
      print("DocBot: "+answer)

    #Search in corpus using TFIDF and cosine similarity
    else:

        #Find the link for the user's query disease
        linkIndex = which_link(user_response)
        path = list(diseasesDict.values())[linkIndex]

        #Generate corpus
        corpus = generateCorpus(path)

        #Tokenisation of corpus
        sent_tokens = sentTokenize(corpus)

        #Recieve response using TFIDF and cosine similarity
        previousIDX,answer = response(user_response,sent_tokens)
        previousResponse = user_response
        print("DocBot: "+answer)
        
  else:
    flag = False
    print("DocBot: Chat with you later!")